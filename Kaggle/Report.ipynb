{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE397K: Data Science Lab\n",
    "### Kaggle Competiton, October 2017\n",
    "### Rachel Chen (rjc2737)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a story of naivety; getting lucky, tuning parameters, trying things the proper way, and then being surprised by the truth in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Lucky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The First Attempt: Straight Jump into XGBoost\n",
    "\n",
    "The first thing I did was look up a [tutorial](https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281/notebook) that provided a quick and dirty solution to my problem. I wanted to shotgun to XGBoost and see what it would do. The tutorial uses the gini-metric as an evaluation metric, which is closely related to AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "\n",
    "#Read csv into pandas\n",
    "train = pd.read_csv(\"train_final.csv\")\n",
    "test = pd.read_csv(\"test_final.csv\")\n",
    "\n",
    "#separate the data into variables\n",
    "X = train.drop(['id', 'Y'], axis=1).values\n",
    "y = train.Y\n",
    "test_id = test.id.values\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score\n",
    "\n",
    "#Stratified KFold - used to keep the distribution of each label consistent for each training batch.\n",
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=42)\n",
    "\n",
    "#create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['Y'] = np.zeros_like(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set parameters\n",
    "params = {\n",
    "    'min_child_weight': 10.0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'max_delta_step': 1.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'subsample': 0.8,\n",
    "    'eta': 0.025,\n",
    "    'gamma': 0.65,\n",
    "    'num_boost_round' : 700\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5]\n",
      "[0]\ttrain-error:0.062555\tvalid-error:0.065393\ttrain-gini:0.621462\tvalid-gini:0.566921\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 70 rounds.\n",
      "[100]\ttrain-error:0.06073\tvalid-error:0.064594\ttrain-gini:0.773729\tvalid-gini:0.699121\n",
      "[200]\ttrain-error:0.056729\tvalid-error:0.064294\ttrain-gini:0.796966\tvalid-gini:0.700259\n",
      "Stopping. Best iteration:\n",
      "[188]\ttrain-error:0.057054\tvalid-error:0.064894\ttrain-gini:0.794211\tvalid-gini:0.700791\n",
      "\n",
      "[Fold 1/5 Prediction:]\n",
      "189\n",
      "[Fold 2/5]\n",
      "[0]\ttrain-error:0.065053\tvalid-error:0.0665\ttrain-gini:0.612306\tvalid-gini:0.614263\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 70 rounds.\n",
      "[100]\ttrain-error:0.061728\tvalid-error:0.0642\ttrain-gini:0.769528\tvalid-gini:0.725185\n",
      "Stopping. Best iteration:\n",
      "[42]\ttrain-error:0.064978\tvalid-error:0.0652\ttrain-gini:0.751387\tvalid-gini:0.726558\n",
      "\n",
      "[Fold 2/5 Prediction:]\n",
      "43\n",
      "[Fold 3/5]\n",
      "[0]\ttrain-error:0.066527\tvalid-error:0.067807\ttrain-gini:0.51521\tvalid-gini:0.524037\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 70 rounds.\n",
      "[100]\ttrain-error:0.061652\tvalid-error:0.066007\ttrain-gini:0.765866\tvalid-gini:0.736049\n",
      "Stopping. Best iteration:\n",
      "[103]\ttrain-error:0.061477\tvalid-error:0.066107\ttrain-gini:0.76642\tvalid-gini:0.736714\n",
      "\n",
      "[Fold 3/5 Prediction:]\n",
      "104\n",
      "[Fold 4/5]\n",
      "[0]\ttrain-error:0.066302\tvalid-error:0.068007\ttrain-gini:0.512194\tvalid-gini:0.522096\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 70 rounds.\n",
      "[100]\ttrain-error:0.062002\tvalid-error:0.065307\ttrain-gini:0.766955\tvalid-gini:0.734282\n",
      "[200]\ttrain-error:0.057751\tvalid-error:0.064106\ttrain-gini:0.788896\tvalid-gini:0.736157\n",
      "[300]\ttrain-error:0.054926\tvalid-error:0.063906\ttrain-gini:0.807607\tvalid-gini:0.737167\n",
      "Stopping. Best iteration:\n",
      "[298]\ttrain-error:0.055026\tvalid-error:0.064206\ttrain-gini:0.807204\tvalid-gini:0.73736\n",
      "\n",
      "[Fold 4/5 Prediction:]\n",
      "299\n",
      "[Fold 5/5]\n",
      "[0]\ttrain-error:0.066827\tvalid-error:0.066707\ttrain-gini:0.522644\tvalid-gini:0.494498\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 70 rounds.\n",
      "[100]\ttrain-error:0.061352\tvalid-error:0.064906\ttrain-gini:0.773586\tvalid-gini:0.701752\n",
      "[200]\ttrain-error:0.057801\tvalid-error:0.065307\ttrain-gini:0.796057\tvalid-gini:0.702696\n",
      "Stopping. Best iteration:\n",
      "[174]\ttrain-error:0.058776\tvalid-error:0.065407\ttrain-gini:0.790804\tvalid-gini:0.703165\n",
      "\n",
      "[Fold 5/5 Prediction:]\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "#XGBoost by stratified kfold \n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    # Convert our data into XGBoost format\n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    d_test = xgb.DMatrix(test.values)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n",
    "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "\n",
    "    print('[Fold %d/%d Prediction:]' % (i + 1, kfold))\n",
    "    \n",
    "    print mdl.best_ntree_limit\n",
    "    # Predict on our test data\n",
    "    #p_test = mdl.predict(d_test)\n",
    "    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n",
    "    sub['Y'] += p_test/kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write to CSV\n",
    "sub.to_csv('Submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result? \n",
    "Amazing. Without preprocessing the data and using the parameters from the tutorial, I had achieved a score of **0.86793** on the public leaderboard! Man, had I found the *right* tutorial to use. To be honest, I didn't completely understand what was going on, but I knew that the key principle was to tune the parameters to better my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Parameters\n",
    "\n",
    "Because I didn't know how to use grid-search yet, I just changed the parameters by hand. [This](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) guide was helpful in understanding the meaning behind the different parameters. \n",
    "\n",
    "My initial attempts consisted of tuning random parameters up/down at random increments but then I realized that doing this without a method would be hard for me to decipher what actually improved my score. I started to document all that I did to see if I could identify how the parameters affected my public leaderboard score. \n",
    "\n",
    "### My method:\n",
    "1. See the results from the previous submission and pick a parameter to tune.\n",
    "2. Change the number slightly. If results from the past showed that making this parameter bigger yielder worse results, make the parameter smaller and see what would happen.\n",
    "3. Plug into the pipeline above to generate a submission file.\n",
    "4. Submit and see what the score would be.\n",
    "\n",
    "![](http://i.markdownnotes.com/image_WbP04sd.jpg)\n",
    "*tuning parameters randomly and seeing what would happen*\n",
    "\n",
    "The tutorial recommended to tune min_child_weight and max_depth first because these parameters make the biggest difference. I saw the results of this when I was randomly tuning parameters. So instead of tuning a couple of parameters at the same time, I started on min_child_weight.\n",
    "\n",
    "![](http://i.markdownnotes.com/image_dq5dOLO.jpg)\n",
    "\n",
    "### Tuning\n",
    "Observation:\n",
    "* It was interesting for me to observe that the **score was not a linear function of min_child_weight**. My first attempt at 9.5 was good, then I tried 9.3 which was yielded better results, but 9.2 was not better than 9.3. In order from best to worse performance based on min_child_weight with all other factors held constant: 9.3, 9.5, 9.2, 9.4, 9.35.\n",
    "\n",
    "Takeaway:\n",
    "* Though I did not use grid-search yet, in my hand implementation I saw how the parameters that are available in a grid-search could really make a difference. There are local mins and maxs when the score is a function of min_child_weight and in order to *really* tune parameters, being precise can make a big difference in end score.\n",
    "\n",
    "Other Parameters:\n",
    "\n",
    "After I found that 9.3 was the sweet-spot for min_child_weight, I moved onto tune max_depth, and then messed with eta, gamma, and subsample. To much disappointment, changing these parameters from what I originally used did not improve the score so I stayed with the parameters from the tutorial. Later when I tried to account for overfitting by playing around with the lamdba value but this did not improve my score on the public leaderboard either.\n",
    "\n",
    "Tuning colsample_bytree started to make an improvement. Just like min_child_weight, the score was not a linear function of colsample_bytree. 0.55 yielded the best results, then in decreasing score, 0.5, 0.58. 0.48, and 0.6. I tracked this in my record as noted by the highlighted boxes in the table below.\n",
    "\n",
    "### Blending...ish\n",
    "Another technique that I used was taking the average of the top submissions and seeing how that faired in the public leaderboard. I used the average of [25,36,37,38,24,27,28] and this placed right in the middle of the submissions I took the average of. Nothing too exciting.\n",
    "\n",
    "![](http://i.markdownnotes.com/image_1a2E30f.jpg)\n",
    "\n",
    "### Getting Around the Submissions Limit\n",
    "After submitting a collection of submissions, I pulled up the numbers of the submissions that had faired the best and compared them with other submissions that had not done as well. Below: the left cluster has better scores than the right cluster of columns.\n",
    "![](http://i.markdownnotes.com/image_Kzb0TDC.png)\n",
    "I noted that there was a general area that in which the entries would fall for each sample. As I tested my parameters, I would check thpse results against the results of entries that got top scores. If they were too different then I would not bother submitting them in Kaggle. This strategy helped me test parameters without wasting too many submissions, but it also surely led to overfitting, as I would soon discover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways from Hand-Tuning\n",
    "Hand-tuning, in retrospect, was not the most efficient system for long-term results. Essentially I was doing my own version of grid-search.\n",
    "\n",
    "Pros:\n",
    "* I was able to get a pretty immediate score on Kaggle and move up the leaderboard from my hand-tuning attempts. I did pretty well on the public leaderboard with this method.\n",
    "\n",
    "Cons:\n",
    "* However, this was a pretty laborious process that took most of the week as I just went from (budding) intuition and hand-tuned randomly until I could identify a pattern.\n",
    "* By hand-tuning parameter by parameter, one at a time, I could potentially be missing out on how parameters work together for a score that's better together. For example, if one parameter yielded a low score (let's say parameter A of value 0.5 gave a score of 0.8 but when A is 0.6, yields a score of 0.9) as did another (parameter B of value 0.5 gives a score 0.7, but when it is 0.6 gives a score of 0.8), but when A and B are 0.5 and 0.5, yield a score of 0.95.\n",
    "* Since I did not calculate the local AUC score and banked on the result I got from the public leaderboard, I was in danger of overfitting when I adjusted my parameters based on responses from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try Grid-Search\n",
    "\n",
    "I found a [tutorial](https://cambridgespark.com/content/tutorials/hyperparameter-tuning-in-xgboost/index.html) that had some code for grid-search. I tested it out to see what would happen, using the gini-metric for consistency from my previous method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "\n",
    "#Read csv into pandas\n",
    "train = pd.read_csv(\"train_final.csv\")\n",
    "test = pd.read_csv(\"test_final.csv\")\n",
    "#Verify that it is correct\n",
    "train.head()\n",
    "test.head()\n",
    "\n",
    "X = train.drop(['id', 'Y'], axis=1).values\n",
    "y = train.Y\n",
    "test_id = test.id.values\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.1, random_state=42)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'min_child_weight': 9.3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 8,\n",
    "    'max_delta_step': 2,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'subsample': 0.8,\n",
    "    'eta': 0.02,\n",
    "    'gamma': 0.65,\n",
    "    'num_boost_round' : 700\n",
    "    }\n",
    "\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(5,12,1)\n",
    "    for min_child_weight in range(5,12,1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=5, min_child_weight=5\n",
      "\tGini 0.6434962 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=6\n",
      "\tGini 0.6440618 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=7\n",
      "\tGini 0.644752 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=8\n",
      "\tGini 0.6445916 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=9\n",
      "\tGini 0.6446492 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=10\n",
      "\tGini 0.6448934 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=11\n",
      "\tGini 0.645039 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=5\n",
      "\tGini 0.6439912 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "\tGini 0.643363 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=7\n",
      "\tGini 0.6436786 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=8\n",
      "\tGini 0.6433024 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=9\n",
      "\tGini 0.6432454 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=10\n",
      "\tGini 0.6441316 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=11\n",
      "\tGini 0.6445514 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "\tGini 0.6419868 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "\tGini 0.6422532 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=7\n",
      "\tGini 0.6422058 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=8\n",
      "\tGini 0.6419592 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=9\n",
      "\tGini 0.642729 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=10\n",
      "\tGini 0.6433204 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=11\n",
      "\tGini 0.6443792 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=5\n",
      "\tGini 0.6417814 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=6\n",
      "\tGini 0.6417746 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=7\n",
      "\tGini 0.6418514 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=8\n",
      "\tGini 0.6415866 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=9\n",
      "\tGini 0.6425898 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=10\n",
      "\tGini 0.642989 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=11\n",
      "\tGini 0.644189 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=5\n",
      "\tGini 0.6403866 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tGini 0.6409668 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tGini 0.6400158 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=8\n",
      "\tGini 0.640134 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=9\n",
      "\tGini 0.6411612 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=10\n",
      "\tGini 0.6419956 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=11\n",
      "\tGini 0.643287 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tGini 0.640261 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tGini 0.640892 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "\tGini 0.640167 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=8\n",
      "\tGini 0.6400326 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=9\n",
      "\tGini 0.641161 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=10\n",
      "\tGini 0.6419702 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=11\n",
      "\tGini 0.643205 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tGini 0.6396156 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tGini 0.640249 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tGini 0.6395304 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=8\n",
      "\tGini 0.6394922 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=9\n",
      "\tGini 0.6405992 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=10\n",
      "\tGini 0.641427 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=11\n",
      "\tGini 0.6426376 for 0 rounds\n",
      "Best params: 11, 8, Gini: 0.6394922\n"
     ]
    }
   ],
   "source": [
    "# Define initial best params and gini\n",
    "min_gini = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        feval=gini_xgb,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best gini\n",
    "    mean_gini = cv_results['test-gini-mean'].min()\n",
    "    boost_rounds = cv_results['test-gini-mean'].argmin()\n",
    "    print(\"\\tGini {} for {} rounds\".format(mean_gini, boost_rounds))\n",
    "    if mean_gini < min_gini:\n",
    "        min_gini = mean_gini\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Gini: {}\".format(best_params[0], best_params[1], min_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were not as successful as I had hope. When I had plugged in the parameter results from GridSearch, I did not get a better score, instead, it fell within the bottom half of my total results. This was still the case when I tried with different seeds in splitting up my data. I got different results each time but averaged the numbers for min_child_weight and max_depth and the results were subpar. This incentivized me to keep doing my hand-tuning because the results were faring much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=5, min_child_weight=5\n",
      "\tAUC 0.8237166 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=6\n",
      "\tAUC 0.8240056 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=7\n",
      "\tAUC 0.8243488 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=8\n",
      "\tAUC 0.824265 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=9\n",
      "\tAUC 0.8243008 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=10\n",
      "\tAUC 0.8244192 for 0 rounds\n",
      "CV with max_depth=5, min_child_weight=11\n",
      "\tAUC 0.8244942 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=5\n",
      "\tAUC 0.8239604 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "\tAUC 0.8236588 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=7\n",
      "\tAUC 0.8237926 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=8\n",
      "\tAUC 0.8235952 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=9\n",
      "\tAUC 0.8235738 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=10\n",
      "\tAUC 0.824039 for 0 rounds\n",
      "CV with max_depth=6, min_child_weight=11\n",
      "\tAUC 0.8242538 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "\tAUC 0.8229846 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "\tAUC 0.8231364 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=7\n",
      "\tAUC 0.823055 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=8\n",
      "\tAUC 0.8229146 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=9\n",
      "\tAUC 0.823307 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=10\n",
      "\tAUC 0.8236576 for 0 rounds\n",
      "CV with max_depth=7, min_child_weight=11\n",
      "\tAUC 0.8242054 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=5\n",
      "\tAUC 0.822881 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=6\n",
      "\tAUC 0.8228824 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=7\n",
      "\tAUC 0.8229354 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=8\n",
      "\tAUC 0.8228038 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=9\n",
      "\tAUC 0.8233112 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=10\n",
      "\tAUC 0.8235168 for 0 rounds\n",
      "CV with max_depth=8, min_child_weight=11\n",
      "\tAUC 0.8241172 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=5\n",
      "\tAUC 0.8222202 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tAUC 0.8225098 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tAUC 0.8220466 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=8\n",
      "\tAUC 0.8220786 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=9\n",
      "\tAUC 0.8225936 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=10\n",
      "\tAUC 0.8230198 for 0 rounds\n",
      "CV with max_depth=9, min_child_weight=11\n",
      "\tAUC 0.8236648 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tAUC 0.8221528 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tAUC 0.8224772 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "\tAUC 0.8221324 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=8\n",
      "\tAUC 0.8220344 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=9\n",
      "\tAUC 0.822598 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=10\n",
      "\tAUC 0.8230222 for 0 rounds\n",
      "CV with max_depth=10, min_child_weight=11\n",
      "\tAUC 0.823636 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tAUC 0.8218604 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tAUC 0.8221816 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tAUC 0.821834 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=8\n",
      "\tAUC 0.82178 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=9\n",
      "\tAUC 0.8223312 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=10\n",
      "\tAUC 0.8227446 for 0 rounds\n",
      "CV with max_depth=11, min_child_weight=11\n",
      "\tAUC 0.8233468 for 0 rounds\n",
      "Best params: 11, 8, AUC: 0.82178\n"
     ]
    }
   ],
   "source": [
    "# Define initial best params and auc\n",
    "min_auc = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics='auc',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best auc\n",
    "    mean_auc = cv_results['test-auc-mean'].min()\n",
    "    boost_rounds = cv_results['test-auc-mean'].argmin()\n",
    "    print(\"\\tAUC {} for {} rounds\".format(mean_auc, boost_rounds))\n",
    "    if mean_auc < min_auc:\n",
    "        min_auc = mean_auc\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, AUC: {}\".format(best_params[0], best_params[1], min_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In testing with AUC instead of the gini-metric, the results were about the same for min_child_weight and max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Things The Proper Way\n",
    "\n",
    "I knew there was a better way to do things than to just plug in numbers into the parameters of a single XGBoost model. My plan for the next steps were this:\n",
    "\n",
    "1. Data preprocessing - inspect and clean the data.\n",
    "2. Use other models like Logistic Regression and Random Forest Classifier and tune parameters with grid-search.\n",
    "3. Ensemble the results from these models (include XGBoost) and run another XGBoost model on top of that.\n",
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "#Read csv into pandas\n",
    "train = pd.read_csv(\"train_final.csv\")\n",
    "test = pd.read_csv(\"test_final.csv\")\n",
    "#Verify that it is correct\n",
    "train.head()\n",
    "test.head()\n",
    "\n",
    "X = train.drop(['id', 'Y'], axis=1)\n",
    "y = train.Y\n",
    "train_id = train.id.values\n",
    "test_id = test.id.values\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "X_ = X.copy()\n",
    "y_ = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the empty values\n",
    "\n",
    "I knew that some cells were missing, but which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print X_.isnull().sum()\n",
    "print y_.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, just two columns, and thankfully the same columns for each data set. \n",
    "#### Check what kind of data each column contains. If categorical, replace with mode value. If numeric, replace with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print X_.F5.value_counts()\n",
    "print X_.F19.value_counts()\n",
    "\n",
    "X_.F5 = X_.F5.replace(r'\\s+', np.nan, regex=True).fillna(0)\n",
    "X_.F19 = X_.F19.replace(r'\\s+', np.nan, regex=True).fillna(X_.F19.mean())\n",
    "\n",
    "y_.F5 = y_.F5.replace(r'\\s+', np.nan, regex=True).fillna(0)\n",
    "y_.F19 = y_.F19.replace(r'\\s+', np.nan, regex=True).fillna(y_.F19.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then inspect each feature and determine if it is categorical or numeric data. \n",
    "I inspected the value counts list of each feature. If there was a range of 15 integers or less with distinct bucketizing, then I determined this to be a categorical feature. If the value_counts were not whole numbers and/or displayed value_counts of a series of 1's, then this was a numeric feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     48227\n",
      "2      1484\n",
      "3       218\n",
      "4        43\n",
      "5        15\n",
      "6         8\n",
      "18        1\n",
      "12        1\n",
      "8         1\n",
      "Name: F1, dtype: int64\n",
      "0     47470\n",
      "1      1905\n",
      "2       379\n",
      "3       104\n",
      "98       87\n",
      "4        32\n",
      "5         9\n",
      "6         6\n",
      "96        3\n",
      "11        1\n",
      "9         1\n",
      "7         1\n",
      "Name: F2, dtype: int64\n",
      " 0.281078    1\n",
      "-0.006354    1\n",
      " 0.690687    1\n",
      "-0.139957    1\n",
      " 0.503359    1\n",
      " 0.111420    1\n",
      "-0.102351    1\n",
      "-0.054156    1\n",
      " 0.035681    1\n",
      "-0.138120    1\n",
      " 0.105642    1\n",
      "-0.079776    1\n",
      " 0.266861    1\n",
      " 1.373554    1\n",
      " 0.248705    1\n",
      " 0.811418    1\n",
      " 0.011395    1\n",
      " 0.225882    1\n",
      " 0.912682    1\n",
      "-0.036151    1\n",
      " 0.153582    1\n",
      " 0.362630    1\n",
      " 0.974042    1\n",
      " 0.188194    1\n",
      " 0.053269    1\n",
      " 0.738982    1\n",
      " 0.168041    1\n",
      " 0.881221    1\n",
      " 0.629370    1\n",
      " 0.225687    1\n",
      "            ..\n",
      " 0.013587    1\n",
      " 0.012951    1\n",
      " 0.200457    1\n",
      " 1.473349    1\n",
      " 0.120619    1\n",
      "-0.017810    1\n",
      " 0.230897    1\n",
      " 0.315938    1\n",
      " 0.960116    1\n",
      " 0.062209    1\n",
      " 0.373453    1\n",
      " 0.924025    1\n",
      " 0.456454    1\n",
      "-0.086199    1\n",
      " 0.365165    1\n",
      " 0.562710    1\n",
      "-0.092705    1\n",
      " 1.021033    1\n",
      " 0.443741    1\n",
      " 0.924752    1\n",
      " 1.090826    1\n",
      " 0.562335    1\n",
      " 0.693398    1\n",
      " 1.192811    1\n",
      " 1.023745    1\n",
      " 0.453490    1\n",
      " 0.469817    1\n",
      " 0.185412    1\n",
      " 0.298567    1\n",
      " 0.042580    1\n",
      "Name: F3, Length: 49998, dtype: int64\n",
      "0    48221\n",
      "1     1496\n",
      "2      205\n",
      "3       55\n",
      "5        8\n",
      "4        8\n",
      "7        2\n",
      "6        2\n",
      "8        1\n",
      "Name: F4, dtype: int64\n",
      "0.0     29110\n",
      "1.0      8682\n",
      "2.0      6516\n",
      "3.0      3129\n",
      "4.0       977\n",
      "5.0       232\n",
      "6.0        55\n",
      "7.0        18\n",
      "8.0         8\n",
      "9.0         2\n",
      "10.0        1\n",
      "Name: F5, dtype: int64\n",
      "1         19124\n",
      "2          6900\n",
      "3          3559\n",
      "4          2399\n",
      "5          1640\n",
      "6          1317\n",
      "7          1055\n",
      "8           887\n",
      "9           748\n",
      "10          654\n",
      "11          512\n",
      "13          454\n",
      "12          443\n",
      "14          349\n",
      "15          313\n",
      "17          276\n",
      "19          274\n",
      "16          272\n",
      "18          249\n",
      "20          221\n",
      "21          205\n",
      "22          188\n",
      "23          165\n",
      "24          164\n",
      "25          161\n",
      "27          150\n",
      "26          133\n",
      "29          132\n",
      "28          130\n",
      "30          122\n",
      "          ...  \n",
      "1011          1\n",
      "44038         1\n",
      "1107          1\n",
      "27742         1\n",
      "33891         1\n",
      "60526         1\n",
      "395250        1\n",
      "3891          1\n",
      "754           1\n",
      "18202         1\n",
      "6993          1\n",
      "850           1\n",
      "978           1\n",
      "3059          1\n",
      "13300         1\n",
      "80949         1\n",
      "99394         1\n",
      "40033         1\n",
      "79060         1\n",
      "3315          1\n",
      "1298          1\n",
      "3347          1\n",
      "5456          1\n",
      "9558          1\n",
      "7665          1\n",
      "1522          1\n",
      "3571          1\n",
      "1714          1\n",
      "12023         1\n",
      "2047          1\n",
      "Name: F6, Length: 1880, dtype: int64\n",
      "1     48181\n",
      "2      1521\n",
      "3       223\n",
      "4        40\n",
      "5        16\n",
      "6        11\n",
      "8         4\n",
      "23        1\n",
      "10        1\n",
      "Name: F7, dtype: int64\n",
      "1    48281\n",
      "2     1465\n",
      "3      188\n",
      "4       42\n",
      "5       11\n",
      "6        6\n",
      "8        2\n",
      "7        2\n",
      "9        1\n",
      "Name: F8, dtype: int64\n",
      "1        30398\n",
      "2         7476\n",
      "3         3416\n",
      "4         1942\n",
      "5         1275\n",
      "6          837\n",
      "7          640\n",
      "8          484\n",
      "9          369\n",
      "10         287\n",
      "12         236\n",
      "11         226\n",
      "13         191\n",
      "14         159\n",
      "15         120\n",
      "17         119\n",
      "16         114\n",
      "19          96\n",
      "18          92\n",
      "22          73\n",
      "21          67\n",
      "23          60\n",
      "20          59\n",
      "25          55\n",
      "24          45\n",
      "26          41\n",
      "27          39\n",
      "28          38\n",
      "31          35\n",
      "35          29\n",
      "         ...  \n",
      "712          1\n",
      "552          1\n",
      "520          1\n",
      "456          1\n",
      "360          1\n",
      "177          1\n",
      "264          1\n",
      "790          1\n",
      "106          1\n",
      "566          1\n",
      "171          1\n",
      "245          1\n",
      "405          1\n",
      "501          1\n",
      "5143         1\n",
      "118          1\n",
      "267          1\n",
      "235          1\n",
      "203          1\n",
      "214          1\n",
      "266          1\n",
      "246          1\n",
      "310          1\n",
      "534          1\n",
      "1770         1\n",
      "11535        1\n",
      "1258         1\n",
      "1194         1\n",
      "2315         1\n",
      "4861         1\n",
      "Name: F9, Length: 322, dtype: int64\n",
      "0     18736\n",
      "1     17408\n",
      "2     10498\n",
      "3      2155\n",
      "4       718\n",
      "5       223\n",
      "6       100\n",
      "7        58\n",
      "8        33\n",
      "9        26\n",
      "10       14\n",
      "11        6\n",
      "12        5\n",
      "13        5\n",
      "15        4\n",
      "17        2\n",
      "23        1\n",
      "14        1\n",
      "16        1\n",
      "18        1\n",
      "19        1\n",
      "54        1\n",
      "29        1\n",
      "Name: F10, dtype: int64\n",
      "40    4015\n",
      "41    3920\n",
      "39    3884\n",
      "38    3803\n",
      "42    3659\n",
      "43    3315\n",
      "37    3313\n",
      "44    2935\n",
      "36    2915\n",
      "45    2441\n",
      "35    2343\n",
      "34    1954\n",
      "46    1930\n",
      "33    1511\n",
      "47    1461\n",
      "32    1125\n",
      "48    1090\n",
      "31     773\n",
      "49     748\n",
      "50     612\n",
      "30     526\n",
      "51     336\n",
      "29     307\n",
      "52     242\n",
      "28     212\n",
      "27     142\n",
      "53     135\n",
      "26      89\n",
      "54      70\n",
      "55      54\n",
      "25      39\n",
      "24      23\n",
      "56      23\n",
      "57      18\n",
      "23      10\n",
      "21       6\n",
      "58       5\n",
      "59       5\n",
      "22       5\n",
      "63       1\n",
      "18       1\n",
      "61       1\n",
      "20       1\n",
      "Name: F11, dtype: int64\n",
      "1     48157\n",
      "2      1556\n",
      "3       197\n",
      "4        57\n",
      "5        20\n",
      "6         6\n",
      "7         3\n",
      "12        1\n",
      "11        1\n",
      "Name: F12, dtype: int64\n",
      "1     48126\n",
      "2      1588\n",
      "3       214\n",
      "4        40\n",
      "5        14\n",
      "6        10\n",
      "9         2\n",
      "7         2\n",
      "10        1\n",
      "8         1\n",
      "Name: F13, dtype: int64\n",
      "0     47176\n",
      "1      1783\n",
      "2       519\n",
      "3       227\n",
      "4        95\n",
      "98       87\n",
      "5        46\n",
      "6        24\n",
      "9        11\n",
      "7        11\n",
      "8         9\n",
      "10        3\n",
      "96        3\n",
      "11        2\n",
      "14        1\n",
      "13        1\n",
      "Name: F14, dtype: int64\n",
      "1     48194\n",
      "2      1547\n",
      "3       175\n",
      "4        49\n",
      "5        18\n",
      "6         7\n",
      "7         3\n",
      "10        2\n",
      "8         2\n",
      "9         1\n",
      "Name: F15, dtype: int64\n",
      "1.77       30490\n",
      "3.54        7565\n",
      "5.31        3474\n",
      "7.08        1875\n",
      "8.85        1170\n",
      "10.62        810\n",
      "12.39        646\n",
      "14.16        483\n",
      "15.93        365\n",
      "17.70        314\n",
      "21.24        229\n",
      "19.47        222\n",
      "23.01        166\n",
      "24.78        151\n",
      "26.55        113\n",
      "28.32        107\n",
      "31.86         88\n",
      "30.09         87\n",
      "33.63         82\n",
      "35.40         74\n",
      "37.17         68\n",
      "42.48         67\n",
      "40.71         52\n",
      "47.79         50\n",
      "38.94         49\n",
      "46.02         49\n",
      "44.25         48\n",
      "54.87         45\n",
      "49.56         40\n",
      "51.33         33\n",
      "           ...  \n",
      "375.24         1\n",
      "415.95         1\n",
      "3623.19        1\n",
      "256.65         1\n",
      "3389.55        1\n",
      "246.03         1\n",
      "1780.62        1\n",
      "238.95         1\n",
      "327.45         1\n",
      "1015.98        1\n",
      "750.48         1\n",
      "3136.44        1\n",
      "766.41         1\n",
      "787.65         1\n",
      "417.72         1\n",
      "1603.62        1\n",
      "4750.68        1\n",
      "251.34         1\n",
      "2962.98        1\n",
      "242.49         1\n",
      "522.15         1\n",
      "1893.90        1\n",
      "428.34         1\n",
      "313.29         1\n",
      "348.69         1\n",
      "1888.59        1\n",
      "869.07         1\n",
      "346.92         1\n",
      "2849.70        1\n",
      "984.12         1\n",
      "Name: F16, Length: 310, dtype: int64\n",
      "1     48274\n",
      "2      1484\n",
      "3       181\n",
      "4        29\n",
      "5        18\n",
      "6         6\n",
      "7         3\n",
      "8         2\n",
      "10        1\n",
      "Name: F17, dtype: int64\n",
      "125    1313\n",
      "126    1284\n",
      "124    1261\n",
      "123    1254\n",
      "128    1216\n",
      "130    1215\n",
      "131    1214\n",
      "138    1207\n",
      "127    1207\n",
      "139    1203\n",
      "129    1200\n",
      "140    1189\n",
      "135    1189\n",
      "133    1166\n",
      "122    1131\n",
      "134    1128\n",
      "121    1123\n",
      "132    1113\n",
      "120    1086\n",
      "136    1064\n",
      "137    1047\n",
      "118    1034\n",
      "119    1024\n",
      "141    1022\n",
      "117    1009\n",
      "116     973\n",
      "115     855\n",
      "114     850\n",
      "143     850\n",
      "142     849\n",
      "       ... \n",
      "102     317\n",
      "156     310\n",
      "101     292\n",
      "158     289\n",
      "157     269\n",
      "159     209\n",
      "100     197\n",
      "160     180\n",
      "161     180\n",
      "162     176\n",
      "163     152\n",
      "99      129\n",
      "164     112\n",
      "165     111\n",
      "166      90\n",
      "98       69\n",
      "167      68\n",
      "168      43\n",
      "170      36\n",
      "169      28\n",
      "172      14\n",
      "171      12\n",
      "176       6\n",
      "174       6\n",
      "179       3\n",
      "173       3\n",
      "175       2\n",
      "178       1\n",
      "184       1\n",
      "180       1\n",
      "Name: F18, Length: 83, dtype: int64\n",
      "5000.0     908\n",
      "4000.0     704\n",
      "6000.0     669\n",
      "3000.0     608\n",
      "0.0        557\n",
      "2500.0     525\n",
      "10000.0    493\n",
      "3500.0     476\n",
      "7000.0     408\n",
      "4500.0     396\n",
      "8000.0     356\n",
      "2000.0     349\n",
      "7500.0     341\n",
      "10500.0    324\n",
      "5500.0     318\n",
      "6500.0     277\n",
      "9000.0     256\n",
      "1.0        212\n",
      "3600.0     203\n",
      "4200.0     198\n",
      "6250.0     193\n",
      "8333.0     191\n",
      "12000.0    190\n",
      "3750.0     190\n",
      "3200.0     183\n",
      "1500.0     180\n",
      "4166.0     176\n",
      "3333.0     175\n",
      "12500.0    172\n",
      "2800.0     170\n",
      "          ... \n",
      "74872.0      1\n",
      "8569.0       1\n",
      "34920.0      1\n",
      "10078.0      1\n",
      "10832.0      1\n",
      "10925.0      1\n",
      "14383.0      1\n",
      "5915.0       1\n",
      "1634.0       1\n",
      "82000.0      1\n",
      "17860.0      1\n",
      "2537.0       1\n",
      "1387.0       1\n",
      "4744.0       1\n",
      "9772.0       1\n",
      "58000.0      1\n",
      "9172.0       1\n",
      "1460.0       1\n",
      "12444.0      1\n",
      "14854.0      1\n",
      "10615.0      1\n",
      "1107.0       1\n",
      "5856.0       1\n",
      "13425.0      1\n",
      "6005.0       1\n",
      "12387.0      1\n",
      "3831.0       1\n",
      "8784.0       1\n",
      "4024.0       1\n",
      "7687.0       1\n",
      "Name: F19, Length: 8770, dtype: int64\n",
      "1     48159\n",
      "2      1553\n",
      "3       207\n",
      "4        44\n",
      "5        19\n",
      "6        10\n",
      "8         2\n",
      "7         2\n",
      "10        1\n",
      "9         1\n",
      "Name: F20, dtype: int64\n",
      "0        30010\n",
      "1         7735\n",
      "2         3411\n",
      "3         1922\n",
      "4         1229\n",
      "5          873\n",
      "6          652\n",
      "7          498\n",
      "8          401\n",
      "9          304\n",
      "10         257\n",
      "11         225\n",
      "12         205\n",
      "13         146\n",
      "14         144\n",
      "15         114\n",
      "16         110\n",
      "17          97\n",
      "18          93\n",
      "19          74\n",
      "22          69\n",
      "20          66\n",
      "21          60\n",
      "24          54\n",
      "28          50\n",
      "25          48\n",
      "23          43\n",
      "29          41\n",
      "27          35\n",
      "31          34\n",
      "         ...  \n",
      "270          1\n",
      "590          1\n",
      "180          1\n",
      "718          1\n",
      "148          1\n",
      "5329         1\n",
      "95133        1\n",
      "691          1\n",
      "595          1\n",
      "531          1\n",
      "530          1\n",
      "498          1\n",
      "370          1\n",
      "210          1\n",
      "178          1\n",
      "721          1\n",
      "369          1\n",
      "177          1\n",
      "2096         1\n",
      "1328         1\n",
      "912          1\n",
      "240          1\n",
      "176          1\n",
      "1167         1\n",
      "815          1\n",
      "719          1\n",
      "463          1\n",
      "303          1\n",
      "239          1\n",
      "430          1\n",
      "Name: F21, Length: 334, dtype: int64\n",
      "6     4561\n",
      "7     4416\n",
      "5     4398\n",
      "8     4183\n",
      "4     3849\n",
      "9     3742\n",
      "10    3172\n",
      "3     3052\n",
      "11    2766\n",
      "12    2342\n",
      "2     2169\n",
      "13    1911\n",
      "14    1554\n",
      "1     1460\n",
      "15    1237\n",
      "16    1018\n",
      "17     784\n",
      "18     649\n",
      "0      619\n",
      "19     453\n",
      "20     380\n",
      "21     271\n",
      "22     241\n",
      "23     178\n",
      "24     146\n",
      "25     101\n",
      "26      69\n",
      "27      59\n",
      "28      45\n",
      "29      35\n",
      "30      27\n",
      "31      24\n",
      "32      15\n",
      "33      14\n",
      "34      12\n",
      "35       7\n",
      "36       7\n",
      "45       4\n",
      "38       4\n",
      "48       3\n",
      "43       2\n",
      "54       2\n",
      "46       2\n",
      "39       2\n",
      "49       2\n",
      "40       2\n",
      "51       1\n",
      "52       1\n",
      "50       1\n",
      "56       1\n",
      "57       1\n",
      "41       1\n",
      "58       1\n",
      "47       1\n",
      "37       1\n",
      "Name: F22, dtype: int64\n",
      "0.000000    3628\n",
      "1.000000    3398\n",
      "1.000000       6\n",
      "0.161677       4\n",
      "0.046048       3\n",
      "0.441118       3\n",
      "0.948207       3\n",
      "0.073528       3\n",
      "0.001129       3\n",
      "0.962076       3\n",
      "0.018798       3\n",
      "0.279441       3\n",
      "0.002500       3\n",
      "0.024776       3\n",
      "1.007798       3\n",
      "0.002400       3\n",
      "0.830565       3\n",
      "0.064344       3\n",
      "1.099800       3\n",
      "0.070493       3\n",
      "0.207379       3\n",
      "0.001750       3\n",
      "0.543457       3\n",
      "0.713147       2\n",
      "0.768846       2\n",
      "0.001950       2\n",
      "0.017049       2\n",
      "0.215569       2\n",
      "1.043912       2\n",
      "0.128492       2\n",
      "            ... \n",
      "0.029598       1\n",
      "0.318442       1\n",
      "0.074070       1\n",
      "0.481158       1\n",
      "0.031613       1\n",
      "0.173306       1\n",
      "0.904419       1\n",
      "0.004539       1\n",
      "0.246203       1\n",
      "0.035697       1\n",
      "0.217286       1\n",
      "0.181799       1\n",
      "0.013984       1\n",
      "1.020705       1\n",
      "0.001981       1\n",
      "0.307701       1\n",
      "0.921436       1\n",
      "0.404757       1\n",
      "0.031543       1\n",
      "0.000800       1\n",
      "0.031884       1\n",
      "0.203611       1\n",
      "0.006500       1\n",
      "0.950420       1\n",
      "0.158295       1\n",
      "0.339012       1\n",
      "0.014812       1\n",
      "0.255255       1\n",
      "0.960577       1\n",
      "0.174445       1\n",
      "Name: F23, Length: 42562, dtype: int64\n",
      "1     48228\n",
      "2      1510\n",
      "3       169\n",
      "4        63\n",
      "5        14\n",
      "6         8\n",
      "7         3\n",
      "8         2\n",
      "10        1\n",
      "Name: F24, dtype: int64\n",
      "0     41952\n",
      "1      5363\n",
      "2      1554\n",
      "3       606\n",
      "4       242\n",
      "5       108\n",
      "98       87\n",
      "6        44\n",
      "7        24\n",
      "8         8\n",
      "9         4\n",
      "96        3\n",
      "12        2\n",
      "13        1\n",
      "Name: F25, dtype: int64\n",
      "48     1313\n",
      "49     1284\n",
      "47     1261\n",
      "46     1254\n",
      "51     1216\n",
      "53     1215\n",
      "54     1214\n",
      "61     1207\n",
      "50     1207\n",
      "62     1203\n",
      "52     1200\n",
      "58     1189\n",
      "63     1189\n",
      "56     1166\n",
      "45     1131\n",
      "57     1128\n",
      "44     1123\n",
      "55     1113\n",
      "43     1086\n",
      "59     1064\n",
      "60     1047\n",
      "41     1034\n",
      "42     1024\n",
      "64     1022\n",
      "40     1009\n",
      "39      973\n",
      "38      855\n",
      "66      850\n",
      "37      850\n",
      "65      849\n",
      "       ... \n",
      "25      317\n",
      "79      310\n",
      "24      292\n",
      "81      289\n",
      "80      269\n",
      "82      209\n",
      "23      197\n",
      "83      180\n",
      "84      180\n",
      "85      176\n",
      "86      152\n",
      "22      129\n",
      "87      112\n",
      "88      111\n",
      "89       90\n",
      "21       69\n",
      "90       68\n",
      "91       43\n",
      "93       36\n",
      "92       28\n",
      "95       14\n",
      "94       12\n",
      "99        6\n",
      "97        6\n",
      "102       3\n",
      "96        3\n",
      "98        2\n",
      "101       1\n",
      "103       1\n",
      "107       1\n",
      "Name: F26, Length: 83, dtype: int64\n",
      "0.000000       1342\n",
      "1.000000         74\n",
      "4.000000         61\n",
      "2.000000         54\n",
      "3.000000         52\n",
      "13.000000        50\n",
      "7.000000         44\n",
      "5.000000         44\n",
      "12.000000        43\n",
      "9.000000         38\n",
      "21.000000        37\n",
      "10.000000        37\n",
      "8.000000         35\n",
      "6.000000         34\n",
      "29.000000        33\n",
      "27.000000        32\n",
      "28.000000        32\n",
      "19.000000        32\n",
      "11.000000        32\n",
      "35.000000        29\n",
      "24.000000        29\n",
      "14.000000        28\n",
      "20.000000        27\n",
      "16.000000        25\n",
      "18.000000        25\n",
      "23.000000        25\n",
      "15.000000        24\n",
      "17.000000        24\n",
      "30.000000        24\n",
      "43.000000        22\n",
      "               ... \n",
      "0.628674          1\n",
      "0.652987          1\n",
      "1.167851          1\n",
      "0.674092          1\n",
      "0.620965          1\n",
      "0.154133          1\n",
      "0.149786          1\n",
      "0.157435          1\n",
      "0.152739          1\n",
      "0.422033          1\n",
      "0.332050          1\n",
      "0.031845          1\n",
      "0.172207          1\n",
      "3776.000000       1\n",
      "0.353329          1\n",
      "0.548863          1\n",
      "0.472513          1\n",
      "0.635497          1\n",
      "0.154923          1\n",
      "0.444334          1\n",
      "0.196319          1\n",
      "0.578084          1\n",
      "0.063397          1\n",
      "0.582070          1\n",
      "4224.000000       1\n",
      "0.383801          1\n",
      "0.429688          1\n",
      "0.660421          1\n",
      "0.197366          1\n",
      "0.216586          1\n",
      "Name: F27, Length: 41705, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_.columns)):\n",
    "    print X['F'+str(i+1)].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Breakdown: categorical/numeric\n",
    "F1: categorical <br />\n",
    "F2: categorical <br />\n",
    "F3: numberic <br />\n",
    "F4: categorical <br />\n",
    "F5: categorical <br />\n",
    "F6: numeric <br />\n",
    "F7: categorical <br />\n",
    "F8: categorical <br />\n",
    "F9: numeric <br />\n",
    "F10: categorical <br />\n",
    "F11: numeric <br />\n",
    "F12: categorical <br />\n",
    "F13: categorical <br />\n",
    "F14: categorical <br />\n",
    "F15: categorical <br />\n",
    "F16: numeric <br />\n",
    "F17: categorical <br />\n",
    "F18: numeric <br />\n",
    "F19: numeric <br />\n",
    "F20: categorical <br />\n",
    "F21: numeric <br />\n",
    "F22: numeric <br />\n",
    "F23: numeric <br />\n",
    "F24: categorical <br />\n",
    "F25: categorical <br />\n",
    "F26: numeric <br />\n",
    "F27: numeric <br />\n",
    "\n",
    "#### With the categorical/numeric distinctions, I one-hot encoded categorical data,  and log transformed numeric data if it was skewed.\n",
    "To note, I had to combine both the test and train sets into one data set to one-hot encode the categorical features. On my first attempt when I did the test and train set separately, the size of the the resulting one-hot encoded tables did not match up. Upon realizing that one set might hold more categories than the other, I realized I had to combine both sets together.\n",
    "For numerical data I checked if the contents were skewed more than 0.75. If so, I normalized the numeric data by applying a log transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rachel Chen\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Rachel Chen\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             F3        F6        F9  F11       F16  F18        F19       F21  \\\n",
      "1      0.102174  0.693147  0.693147   32  1.018847  104  10.039023  0.000000   \n",
      "2      0.133093  2.079442  0.693147   44  1.842136  144   9.341456  0.000000   \n",
      "3      0.400330  8.344267  1.386294   32  1.018847  112   8.483430  0.000000   \n",
      "4     -0.054486  1.098612  1.791759   46  1.512927  127   8.086718  0.693147   \n",
      "5      0.548582  2.484907  0.693147   35  1.018847  148   8.294300  0.693147   \n",
      "6      0.332169  4.356709  0.693147   44  1.842136  122   9.201905  1.386294   \n",
      "7     -0.139516  1.945910  1.098612   40  1.018847  120   9.251194  1.386294   \n",
      "8      0.737748  1.945910  0.693147   45  2.287471  113   8.732466  0.000000   \n",
      "9      0.056452  2.397895  0.693147   42  1.018847  110   8.006701  0.000000   \n",
      "10     0.751011  0.693147  3.178054   45  1.842136  127   8.804793  0.000000   \n",
      "11     0.178506  1.386294  1.386294   34  1.018847  126   8.483430  0.000000   \n",
      "12    -0.044231  0.693147  1.098612   49  1.842136  122   9.067278  0.000000   \n",
      "13     0.002472  1.098612  0.693147   47  1.018847  102   8.160804  0.000000   \n",
      "14     0.789994  3.465736  0.693147   45  1.018847  100   7.378384  0.000000   \n",
      "15     0.106626  1.098612  1.098612   42  2.089392  148   8.537192  0.000000   \n",
      "16     0.368988  0.693147  0.693147   44  2.089392  126   9.095715  2.079442   \n",
      "17     0.384393  2.302585  1.098612   44  1.018847  126   8.925454  0.000000   \n",
      "18     0.181112  3.091042  1.098612   44  1.018847  149   6.440947  0.693147   \n",
      "19     0.509167  1.609438  0.693147   36  1.018847  123   8.294300  0.000000   \n",
      "20     0.041585  0.693147  0.693147   33  1.018847  116   7.824446  0.000000   \n",
      "21     0.062445  0.693147  0.693147   43  1.018847  109   8.853808  0.000000   \n",
      "22     0.086343  2.484907  0.693147   45  1.018847  124  11.225257  1.098612   \n",
      "23     0.166152  1.609438  0.693147   43  1.018847  132   8.913012  0.000000   \n",
      "24     0.649400  0.693147  0.693147   41  1.018847  131   9.268704  1.791759   \n",
      "25     0.324091  0.693147  0.693147   39  1.018847  113   7.881937  0.000000   \n",
      "26     0.177888  0.693147  1.098612   44  1.512927  132   9.218705  0.693147   \n",
      "27    -0.123733  1.386294  1.098612   31  1.018847  142   8.973225  0.000000   \n",
      "28     0.103071  4.127134  0.693147   39  4.023028  127   0.000000  0.000000   \n",
      "29    -0.044402  0.693147  0.693147   45  1.018847  149   8.243019  0.000000   \n",
      "30     0.653083  0.693147  0.693147   32  1.842136  100   6.311735  0.000000   \n",
      "...         ...       ...       ...  ...       ...  ...        ...       ...   \n",
      "49969  0.137569  4.189655  0.693147   49  1.512927  125   9.290445  0.000000   \n",
      "49970 -0.011195  4.543295  1.098612   43  1.512927  119   8.824825  0.000000   \n",
      "49971  0.557809  1.098612  1.386294   35  2.928524  116   8.294300  0.000000   \n",
      "49972  0.021235  3.737670  0.693147   43  2.287471  129   9.472782  0.000000   \n",
      "49973  0.149823  1.098612  0.693147   40  1.018847  138   8.804793  0.000000   \n",
      "49974 -0.212822  0.693147  0.693147   49  3.812203  123   8.412055  1.386294   \n",
      "49975  0.200054  2.397895  4.262680   41  1.018847  149   8.804793  0.693147   \n",
      "49976  0.186596  0.693147  1.609438   37  1.018847  139   9.259226  0.000000   \n",
      "49977 -0.186340  0.693147  0.693147   43  1.018847  137   8.665786  0.693147   \n",
      "49978  0.664981  4.499810  2.197225   36  2.089392  129   8.829665  1.098612   \n",
      "49979  0.381848  1.609438  0.693147   40  1.018847  123   8.006701  0.000000   \n",
      "49980  0.684207  0.693147  2.197225   33  1.018847   99   8.804793  0.000000   \n",
      "49981  0.420966  0.693147  1.098612   43  1.842136  126   8.922792  0.000000   \n",
      "49982 -0.066671  0.693147  0.693147   47  1.018847  121   8.974745  0.000000   \n",
      "49983  0.110488  8.855806  0.693147   48  1.018847  110   8.781555  0.000000   \n",
      "49984  0.613850  0.693147  0.693147   45  2.287471  122   7.090910  0.000000   \n",
      "49985  0.161672  0.693147  0.693147   52  1.018847  125   9.073260  0.000000   \n",
      "49986  0.048593  2.484907  0.693147   47  1.018847  115   8.076205  0.000000   \n",
      "49987  0.050576  0.693147  0.693147   48  1.018847  140   8.804793  1.098612   \n",
      "49988 -0.070106  2.833213  2.079442   36  1.512927  136   7.955776  0.000000   \n",
      "49989  0.333629  1.098612  0.693147   43  1.018847  134   8.804793  0.000000   \n",
      "49990  0.099210  1.609438  0.693147   46  4.273884  111   8.343078  0.000000   \n",
      "49991  0.050739  1.098612  1.609438   37  1.018847  122   9.323758  0.000000   \n",
      "49992  0.518270  0.693147  0.693147   41  1.018847  109   6.988413  1.609438   \n",
      "49993  0.175107  2.772589  0.693147   43  1.842136  134   8.557567  0.000000   \n",
      "49994  0.159625  3.044522  1.386294   39  1.018847  132   8.909370  0.000000   \n",
      "49995  0.420438  1.609438  0.693147   38  1.018847  140   9.259226  0.693147   \n",
      "49996  0.295168  0.693147  0.693147   40  1.018847  133   8.464214  0.000000   \n",
      "49997  0.022814  1.945910  2.079442   38  1.018847  114   9.210440  0.000000   \n",
      "49998  0.048296  0.693147  0.693147   46  1.018847  155   8.804793  0.693147   \n",
      "\n",
      "            F22       F23   ...    F25_5  F25_6  F25_7  F25_8  F25_9  F25_10  \\\n",
      "1      2.944439  0.041425   ...        0      0      0      0      0       0   \n",
      "2      2.197225  0.021191   ...        0      0      0      0      0       0   \n",
      "3      2.639057  0.406939   ...        0      0      0      0      0       0   \n",
      "4      2.197225  0.000000   ...        0      0      0      0      0       0   \n",
      "5      1.791759  0.580869   ...        0      0      0      0      0       0   \n",
      "6      2.302585  0.212056   ...        0      0      0      0      0       0   \n",
      "7      1.791759  0.000000   ...        0      0      0      0      0       0   \n",
      "8      0.693147  0.693147   ...        0      0      0      0      0       0   \n",
      "9      1.945910  0.115760   ...        0      0      0      0      0       0   \n",
      "10     2.890372  0.825817   ...        0      0      0      0      0       0   \n",
      "11     2.484907  0.061070   ...        0      0      0      0      0       0   \n",
      "12     2.079442  0.003775   ...        0      0      0      0      0       0   \n",
      "13     1.386294  0.037099   ...        0      0      0      0      0       0   \n",
      "14     1.791759  0.693147   ...        0      0      0      0      0       0   \n",
      "15     1.945910  0.050111   ...        0      0      0      0      0       0   \n",
      "16     2.197225  0.331768   ...        0      0      0      0      0       0   \n",
      "17     1.609438  0.324649   ...        0      0      0      0      0       0   \n",
      "18     2.397895  0.100586   ...        0      0      0      0      0       0   \n",
      "19     2.708050  0.365809   ...        0      0      0      0      0       0   \n",
      "20     2.079442  0.000000   ...        0      0      0      0      0       0   \n",
      "21     1.791759  0.064518   ...        0      0      0      0      0       0   \n",
      "22     1.945910  0.097381   ...        0      0      0      0      0       0   \n",
      "23     2.484907  0.232199   ...        0      0      0      0      0       0   \n",
      "24     2.302585  0.688459   ...        0      0      0      0      0       0   \n",
      "25     1.791759  0.418986   ...        0      0      0      0      0       0   \n",
      "26     1.386294  0.235851   ...        0      0      0      0      0       0   \n",
      "27     3.135494  0.035351   ...        0      0      0      0      0       0   \n",
      "28     2.833213  0.025890   ...        0      0      0      0      0       0   \n",
      "29     2.079442  0.025764   ...        0      0      0      0      0       0   \n",
      "30     0.693147  0.693147   ...        0      0      0      0      0       0   \n",
      "...         ...       ...   ...      ...    ...    ...    ...    ...     ...   \n",
      "49969  1.791759  0.077549   ...        0      0      0      0      0       0   \n",
      "49970  2.397895  0.000000   ...        0      0      0      0      0       0   \n",
      "49971  1.609438  0.611344   ...        0      0      0      0      0       0   \n",
      "49972  2.397895  0.023632   ...        0      0      0      0      0       0   \n",
      "49973  1.945910  0.006234   ...        0      0      0      0      0       0   \n",
      "49974  1.609438  0.000584   ...        0      0      0      0      0       0   \n",
      "49975  1.791759  0.180962   ...        0      0      0      0      0       0   \n",
      "49976  2.397895  0.016306   ...        0      0      0      0      0       0   \n",
      "49977  2.079442  0.002159   ...        0      0      0      0      0       0   \n",
      "49978  1.791759  0.688530   ...        0      0      0      0      0       0   \n",
      "49979  2.772589  0.390010   ...        0      0      0      0      0       0   \n",
      "49980  1.098612  0.642957   ...        0      0      0      0      0       0   \n",
      "49981  2.302585  0.317198   ...        0      0      0      0      0       0   \n",
      "49982  2.197225  0.034123   ...        0      0      0      0      0       0   \n",
      "49983  1.791759  0.009718   ...        0      0      0      0      0       0   \n",
      "49984  1.386294  0.693147   ...        0      0      0      0      0       0   \n",
      "49985  1.945910  0.092619   ...        0      0      0      0      0       0   \n",
      "49986  2.484907  0.170475   ...        0      0      0      0      0       0   \n",
      "49987  2.197225  0.103768   ...        0      0      0      0      0       0   \n",
      "49988  1.098612  0.000000   ...        0      0      0      0      0       0   \n",
      "49989  2.197225  0.228329   ...        0      0      0      0      0       0   \n",
      "49990  1.609438  0.000000   ...        0      0      0      0      0       0   \n",
      "49991  3.091042  0.014850   ...        0      0      0      0      0       0   \n",
      "49992  1.791759  0.541208   ...        0      0      0      0      0       0   \n",
      "49993  2.397895  0.139886   ...        0      0      0      0      0       0   \n",
      "49994  2.302585  0.070902   ...        0      0      0      0      0       0   \n",
      "49995  2.772589  0.461246   ...        0      0      0      0      0       0   \n",
      "49996  3.044522  0.312862   ...        0      0      0      0      0       0   \n",
      "49997  2.484907  0.094501   ...        0      0      0      0      0       0   \n",
      "49998  1.098612  0.002099   ...        0      0      0      0      0       0   \n",
      "\n",
      "       F25_12  F25_13  F25_96  F25_98  \n",
      "1           0       0       0       0  \n",
      "2           0       0       0       0  \n",
      "3           0       0       0       0  \n",
      "4           0       0       0       0  \n",
      "5           0       0       0       0  \n",
      "6           0       0       0       0  \n",
      "7           0       0       0       0  \n",
      "8           0       0       0       0  \n",
      "9           0       0       0       0  \n",
      "10          0       0       0       0  \n",
      "11          0       0       0       0  \n",
      "12          0       0       0       0  \n",
      "13          0       0       0       0  \n",
      "14          0       0       0       0  \n",
      "15          0       0       0       0  \n",
      "16          0       0       0       0  \n",
      "17          0       0       0       0  \n",
      "18          0       0       0       0  \n",
      "19          0       0       0       0  \n",
      "20          0       0       0       0  \n",
      "21          0       0       0       0  \n",
      "22          0       0       0       0  \n",
      "23          0       0       0       0  \n",
      "24          0       0       0       0  \n",
      "25          0       0       0       0  \n",
      "26          0       0       0       0  \n",
      "27          0       0       0       0  \n",
      "28          0       0       0       0  \n",
      "29          0       0       0       0  \n",
      "30          0       0       0       0  \n",
      "...       ...     ...     ...     ...  \n",
      "49969       0       0       0       0  \n",
      "49970       0       0       0       0  \n",
      "49971       0       0       0       0  \n",
      "49972       0       0       0       0  \n",
      "49973       0       0       0       0  \n",
      "49974       0       0       0       0  \n",
      "49975       0       0       0       0  \n",
      "49976       0       0       0       0  \n",
      "49977       0       0       0       0  \n",
      "49978       0       0       0       0  \n",
      "49979       0       0       0       0  \n",
      "49980       0       0       0       0  \n",
      "49981       0       0       0       0  \n",
      "49982       0       0       0       0  \n",
      "49983       0       0       0       0  \n",
      "49984       0       0       0       0  \n",
      "49985       0       0       0       0  \n",
      "49986       0       0       0       0  \n",
      "49987       0       0       0       0  \n",
      "49988       0       0       0       0  \n",
      "49989       0       0       0       0  \n",
      "49990       0       0       0       0  \n",
      "49991       0       0       0       0  \n",
      "49992       0       0       0       0  \n",
      "49993       0       0       0       0  \n",
      "49994       0       0       0       0  \n",
      "49995       0       0       0       0  \n",
      "49996       0       0       0       0  \n",
      "49997       0       0       0       0  \n",
      "49998       0       0       0       0  \n",
      "\n",
      "[49998 rows x 207 columns]\n",
      "             F3         F6        F9  F11       F16  F18       F19       F21  \\\n",
      "49999  0.378640   0.693147  0.693147   35  1.842136  127  8.520587  0.000000   \n",
      "50000  0.106675   1.098612  0.693147   40  1.512927  137  9.024131  1.791759   \n",
      "50001  0.059646   3.401197  1.386294   34  1.018847  142  8.797112  1.791759   \n",
      "50002  0.420926   0.693147  0.693147   42  3.772301  160  7.824446  0.693147   \n",
      "50003  0.461702  10.696231  2.079442   38  1.018847  106  8.919052  1.609438   \n",
      "50004  0.143549   2.397895  0.693147   39  5.379621  129  8.517393  1.791759   \n",
      "50005  0.268482   0.693147  0.693147   37  2.089392  115  8.294300  0.693147   \n",
      "50006  0.223093   1.945910  0.693147   41  1.018847  110  7.978311  0.000000   \n",
      "50007  0.221834   3.401197  1.791759   32  4.170070  110  8.797112  0.000000   \n",
      "50008  0.357624   0.693147  1.386294   41  2.089392  143  8.955577  0.000000   \n",
      "50009  0.189901   0.693147  0.693147   38  2.594508  119  9.210440  1.609438   \n",
      "50010  0.701776   5.068904  1.098612   41  1.512927  114  8.769973  0.000000   \n",
      "50011 -0.014444   1.791759  2.708050   36  1.842136  125  8.366603  0.000000   \n",
      "50012  0.417871   0.693147  0.693147   40  1.018847  127  9.462421  0.000000   \n",
      "50013  0.429858   3.526361  1.386294   39  1.018847  143  8.670944  1.386294   \n",
      "50014  0.540483   0.693147  0.693147   44  1.018847  141  9.698552  1.945910   \n",
      "50015 -0.192093   0.693147  0.693147   44  1.018847  137  9.615872  0.000000   \n",
      "50016  0.334351   0.693147  0.693147   40  1.842136  111  8.687948  0.000000   \n",
      "50017  0.755000   1.386294  1.098612   38  1.512927  135  9.581973  0.000000   \n",
      "50018  0.689053   5.517453  0.693147   38  1.018847  129  8.797112  0.693147   \n",
      "50019  0.095373   1.098612  1.098612   35  1.018847  113  8.642592  1.386294   \n",
      "50020 -0.070828   2.079442  0.693147   39  1.018847  123  8.228444  4.454347   \n",
      "50021  0.160280   2.197225  0.693147   34  1.018847  126  7.790282  0.693147   \n",
      "50022  0.001815   3.401197  0.693147   42  1.018847  135  8.517393  0.693147   \n",
      "50023  0.191444   3.526361  0.693147   32  1.018847  134  8.154788  0.000000   \n",
      "50024  0.284626   0.693147  0.693147   38  1.018847  144  8.265136  0.000000   \n",
      "50025  0.228087   1.098612  0.693147   39  1.512927  117  8.597297  0.000000   \n",
      "50026  0.604899   0.693147  0.693147   43  1.018847  125  8.188967  1.791759   \n",
      "50027 -0.128999   1.791759  0.693147   36  1.842136  160  8.797112  0.000000   \n",
      "50028  0.185544   1.386294  1.609438   36  1.018847  134  8.841737  0.000000   \n",
      "...         ...        ...       ...  ...       ...  ...       ...       ...   \n",
      "99969  0.780478   1.386294  0.693147   36  1.018847  127  8.160804  3.178054   \n",
      "99970  0.579488   0.693147  1.386294   42  1.018847  131  8.782783  0.000000   \n",
      "99971 -0.103559   3.828641  1.791759   46  1.018847  113  8.797112  1.386294   \n",
      "99972  0.187485   0.693147  1.098612   43  1.018847  127  8.343078  1.791759   \n",
      "99973  0.664412   1.609438  0.693147   38  1.018847  135  8.819961  0.000000   \n",
      "99974  0.139904   1.791759  1.386294   44  1.018847  116  8.160804  0.000000   \n",
      "99975  0.482282   0.693147  2.484907   40  1.018847  159  7.424165  1.386294   \n",
      "99976  0.564890   1.791759  0.693147   33  1.018847  121  8.131825  0.000000   \n",
      "99977  0.683613   2.708050  1.098612   43  1.018847  150  8.229511  0.000000   \n",
      "99978 -0.142329   1.609438  0.693147   32  1.018847  134  9.290445  1.098612   \n",
      "99979  0.672270   4.234107  1.386294   37  2.452728  126  8.797112  0.000000   \n",
      "99980  0.175256   0.693147  0.693147   39  3.178470  100  6.835185  2.708050   \n",
      "99981 -0.178975   3.332205  0.693147   36  1.018847   99  6.710523  0.000000   \n",
      "99982  0.288027   0.693147  1.386294   41  1.018847  136  8.881975  1.098612   \n",
      "99983 -0.012900   0.693147  0.693147   41  1.018847  130  8.131825  0.000000   \n",
      "99984  0.257081   0.693147  0.693147   34  1.018847  125  8.578665  0.000000   \n",
      "99985  0.094573   0.693147  1.386294   42  1.018847  123  7.901377  0.000000   \n",
      "99986  0.111281   1.791759  0.693147   52  1.018847  104  8.184235  0.000000   \n",
      "99987  0.053663   3.433987  1.098612   39  2.089392  127  9.159152  0.000000   \n",
      "99988  0.133441   4.248495  0.693147   36  1.018847  147  8.797112  1.098612   \n",
      "99989 -0.144001   0.693147  0.693147   51  1.018847  144  8.797112  0.000000   \n",
      "99990  0.270222   0.693147  0.693147   41  1.018847  140  8.797112  0.000000   \n",
      "99991  0.141444   1.386294  0.693147   39  1.018847  116  8.917445  0.693147   \n",
      "99992  0.374587   0.693147  1.098612   41  2.718660  144  8.973478  0.000000   \n",
      "99993  0.404738   3.433987  1.098612   38  1.018847  133  9.597709  0.000000   \n",
      "99994  0.124398   1.098612  2.564949   42  1.512927  116  8.146419  0.000000   \n",
      "99995  0.572570   1.386294  0.693147   42  1.842136  123  9.047939  0.693147   \n",
      "99996  0.317497   2.564949  1.098612   36  1.018847  121  8.537192  1.609438   \n",
      "99997  0.372084   0.693147  0.693147   43  1.018847  132  8.160804  0.000000   \n",
      "99998  0.338839   2.397895  1.791759   48  1.018847  139  8.334952  0.000000   \n",
      "\n",
      "            F22       F23   ...    F25_5  F25_6  F25_7  F25_8  F25_9  F25_10  \\\n",
      "49999  2.197225  0.340723   ...        0      0      0      0      0       0   \n",
      "50000  1.791759  0.096002   ...        0      0      0      0      0       0   \n",
      "50001  1.609438  0.061185   ...        0      0      0      0      0       0   \n",
      "50002  2.079442  0.428057   ...        0      0      0      0      0       0   \n",
      "50003  2.564949  0.565181   ...        0      0      0      0      0       0   \n",
      "50004  2.833213  0.176419   ...        0      0      0      0      0       0   \n",
      "50005  2.397895  0.206041   ...        0      0      0      0      0       0   \n",
      "50006  1.609438  0.224076   ...        0      0      0      0      0       0   \n",
      "50007  1.945910  0.195919   ...        0      0      0      0      0       0   \n",
      "50008  2.484907  0.317267   ...        0      0      0      0      0       0   \n",
      "50009  1.945910  0.193020   ...        0      0      0      0      0       0   \n",
      "50010  2.197225  0.693147   ...        0      0      0      0      0       0   \n",
      "50011  1.609438  0.004396   ...        0      0      0      0      0       0   \n",
      "50012  2.397895  0.477015   ...        0      0      0      0      0       0   \n",
      "50013  2.302585  0.524996   ...        0      0      0      0      0       0   \n",
      "50014  2.708050  0.523511   ...        0      0      0      0      0       0   \n",
      "50015  2.564949  0.024899   ...        0      0      0      0      0       0   \n",
      "50016  1.609438  0.354871   ...        0      0      0      0      0       0   \n",
      "50017  1.791759  0.693147   ...        0      0      0      0      0       0   \n",
      "50018  2.639057  0.693147   ...        0      0      0      0      0       0   \n",
      "50019  3.044522  0.127894   ...        0      0      0      0      0       0   \n",
      "50020  2.890372  0.094280   ...        0      0      0      0      0       0   \n",
      "50021  2.397895  0.127913   ...        0      0      0      0      0       0   \n",
      "50022  2.639057  0.024907   ...        0      0      0      0      0       0   \n",
      "50023  1.791759  0.000000   ...        0      0      0      0      0       0   \n",
      "50024  2.564949  0.365349   ...        0      0      0      0      0       0   \n",
      "50025  1.945910  0.168746   ...        0      0      0      0      0       0   \n",
      "50026  1.386294  0.499705   ...        0      0      0      0      0       0   \n",
      "50027  1.386294  0.000000   ...        0      0      0      0      0       0   \n",
      "50028  2.772589  0.099420   ...        0      0      0      0      0       0   \n",
      "...         ...       ...   ...      ...    ...    ...    ...    ...     ...   \n",
      "99969  1.791759  0.705962   ...        0      0      0      0      0       0   \n",
      "99970  2.397895  0.545242   ...        0      0      0      0      0       0   \n",
      "99971  1.609438  0.025202   ...        0      0      0      0      0       0   \n",
      "99972  1.098612  0.003593   ...        0      0      0      0      0       0   \n",
      "99973  2.564949  0.682613   ...        0      0      0      0      0       0   \n",
      "99974  2.079442  0.098284   ...        0      0      0      0      0       0   \n",
      "99975  1.386294  0.396025   ...        0      0      0      0      0       0   \n",
      "99976  2.079442  0.490979   ...        0      0      0      0      0       0   \n",
      "99977  1.945910  0.666368   ...        0      0      0      0      0       0   \n",
      "99978  2.484907  0.000000   ...        0      0      0      0      0       0   \n",
      "99979  0.000000  0.693147   ...        0      0      0      0      0       0   \n",
      "99980  1.098612  0.088841   ...        0      0      0      0      0       0   \n",
      "99981  1.098612  0.000000   ...        0      0      0      0      0       0   \n",
      "99982  1.609438  0.099154   ...        0      0      0      0      0       0   \n",
      "99983  2.639057  0.043242   ...        0      0      0      0      0       0   \n",
      "99984  2.197225  0.431199   ...        0      0      0      0      0       0   \n",
      "99985  2.079442  0.085858   ...        0      0      0      0      0       0   \n",
      "99986  1.386294  0.004082   ...        0      0      0      0      0       0   \n",
      "99987  2.484907  0.035149   ...        0      0      0      0      0       0   \n",
      "99988  2.639057  0.091412   ...        0      0      0      0      0       0   \n",
      "99989  1.945910  0.000000   ...        0      0      0      0      0       0   \n",
      "99990  2.079442  0.116267   ...        0      0      0      0      0       0   \n",
      "99991  1.945910  0.226586   ...        0      0      0      0      0       0   \n",
      "99992  3.295837  0.341236   ...        0      0      0      0      0       0   \n",
      "99993  2.397895  0.309702   ...        0      0      0      0      0       0   \n",
      "99994  1.386294  0.189947   ...        0      0      0      0      0       0   \n",
      "99995  1.609438  0.628562   ...        0      0      0      0      0       0   \n",
      "99996  3.218876  0.166208   ...        0      0      0      0      0       0   \n",
      "99997  1.386294  0.201506   ...        0      0      0      0      0       0   \n",
      "99998  2.772589  0.226105   ...        0      0      0      0      0       0   \n",
      "\n",
      "       F25_12  F25_13  F25_96  F25_98  \n",
      "49999       0       0       0       0  \n",
      "50000       0       0       0       0  \n",
      "50001       0       0       0       0  \n",
      "50002       0       0       0       0  \n",
      "50003       0       0       0       0  \n",
      "50004       0       0       0       0  \n",
      "50005       0       0       0       0  \n",
      "50006       0       0       0       0  \n",
      "50007       0       0       0       0  \n",
      "50008       0       0       0       0  \n",
      "50009       0       0       0       0  \n",
      "50010       0       0       0       0  \n",
      "50011       0       0       0       0  \n",
      "50012       0       0       0       0  \n",
      "50013       0       0       0       0  \n",
      "50014       0       0       0       0  \n",
      "50015       0       0       0       0  \n",
      "50016       0       0       0       0  \n",
      "50017       0       0       0       0  \n",
      "50018       0       0       0       0  \n",
      "50019       0       0       0       0  \n",
      "50020       0       0       0       0  \n",
      "50021       0       0       0       0  \n",
      "50022       0       0       0       0  \n",
      "50023       0       0       0       0  \n",
      "50024       0       0       0       0  \n",
      "50025       0       0       0       0  \n",
      "50026       0       0       0       0  \n",
      "50027       0       0       0       0  \n",
      "50028       0       0       0       0  \n",
      "...       ...     ...     ...     ...  \n",
      "99969       0       0       0       0  \n",
      "99970       0       0       0       0  \n",
      "99971       0       0       0       0  \n",
      "99972       0       0       0       0  \n",
      "99973       0       0       0       0  \n",
      "99974       0       0       0       0  \n",
      "99975       0       0       0       0  \n",
      "99976       0       0       0       0  \n",
      "99977       0       0       0       0  \n",
      "99978       0       0       0       0  \n",
      "99979       0       0       0       0  \n",
      "99980       0       0       0       0  \n",
      "99981       0       0       0       0  \n",
      "99982       0       0       0       0  \n",
      "99983       0       0       0       0  \n",
      "99984       0       0       0       0  \n",
      "99985       0       0       0       0  \n",
      "99986       0       0       0       0  \n",
      "99987       0       0       0       0  \n",
      "99988       0       0       0       0  \n",
      "99989       0       0       0       0  \n",
      "99990       0       0       0       0  \n",
      "99991       0       0       0       0  \n",
      "99992       0       0       0       0  \n",
      "99993       0       0       0       0  \n",
      "99994       0       0       0       0  \n",
      "99995       0       0       0       0  \n",
      "99996       0       0       0       0  \n",
      "99997       0       0       0       0  \n",
      "99998       0       0       0       0  \n",
      "\n",
      "[50000 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.concat([X_, y_], ignore_index=True)\n",
    "\n",
    "categorical_array_ = [2, 4, 5, 7, 8, 10, 12, 13, 14, 15, 17, 20, 24, 25] #remember 1\n",
    "\n",
    "F1_OH = pd.get_dummies(all_data['F1'], prefix='F1')\n",
    "all_data = all_data.join(F1_OH).drop(['F1'], axis = 1)\n",
    "\n",
    "for feat in categorical_array_:\n",
    "    term = str('F'+str(feat))\n",
    "    dummies = pd.get_dummies(all_data[term], prefix=term)\n",
    "    all_data = all_data.join(dummies).drop([term], axis = 1)\n",
    "\n",
    "processed_X = all_data.iloc[:49998, :]\n",
    "processed_y = all_data.iloc[49998:, :]\n",
    "\n",
    "processed_X['idx'] = train_id\n",
    "processed_X = processed_X.set_index('idx')\n",
    "processed_X.index.name = None\n",
    "processed_y['idx'] = test_id\n",
    "processed_y = processed_y.set_index('idx')\n",
    "processed_y.index.name = None\n",
    "\n",
    "# log transfrom skewed numeric features\n",
    "# https://www.kaggle.com/apapiu/regularized-linear-models\n",
    "numeric_array = [3, 6, 9, 11, 16, 18, 19, 21, 22, 23, 26, 27]\n",
    "\n",
    "for feat in numeric_array:\n",
    "    term = str('F'+str(feat))\n",
    "    if processed_X[term].skew() > 0.75:\n",
    "        processed_X[term] = np.log1p(processed_X[term])\n",
    "    if processed_y[term].skew() > 0.75:\n",
    "        processed_y[term] = np.log1p(processed_y[term])\n",
    "        \n",
    "print processed_X\n",
    "print processed_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction\n",
    "\n",
    "Now with 207 columns I thought about applying feature reduction with PCA or forward/backward feature selection. I realize that reducing and eliminating highly correlated features may have improved the results; however, 207 columns is not that much to process through so I decided to skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models: Logistic Regression, Random Forests\n",
    "\n",
    "I wanted to try other models besides XGBoost to see how they would perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "from sklearn.model_selection import cross_val_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Using [this](https://stackoverflow.com/questions/40667856/why-doesnt-gridsearchcv-give-c-with-highest-auc-when-scoring-roc-auc-in-logisti) example, I implemented Logistic Regression. First I did a grid-search to find the best C parameter. The one that performed the best of a value of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.63363, std: 0.00874, params: {'C': 0.001},\n",
       "  mean: 0.82948, std: 0.00818, params: {'C': 0.01},\n",
       "  mean: 0.83590, std: 0.00853, params: {'C': 0.1},\n",
       "  mean: 0.83521, std: 0.00907, params: {'C': 1},\n",
       "  mean: 0.83263, std: 0.00895, params: {'C': 10},\n",
       "  mean: 0.83088, std: 0.00898, params: {'C': 100},\n",
       "  mean: 0.83030, std: 0.00834, params: {'C': 1000}],\n",
       " {'C': 0.1},\n",
       " 0.8358992195351744)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty = 'l1')\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "clf = GridSearchCV(lr, parameters, scoring='roc_auc', cv = 5, n_jobs=-1, verbose=5)\n",
    "clf.fit(processed_X, y)\n",
    "clf.grid_scores_,clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l1', C=0.1)\n",
    "lr.fit(processed_X, y)\n",
    "ypred_test = lr.predict_proba(processed_y)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['Y'] = ypred_test[:,1]\n",
    "sub.to_csv('SubLog1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "Using that value in my model, I trained it on the train data and predicted the results on the test set. This yielded a score of **0.84145** AUC from Kaggle, much lower from my XGBoost attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Similarly, I used [this](https://stackoverflow.com/questions/30102973/how-to-get-best-estimator-on-gridsearchcv-random-forest-classifier-scikit) and [this](https://www.kaggle.com/giovannibruner/randomforest-with-gridsearchcv) to help me apply the Random Forest model on my train and test set. I used grid-search to find the optimal max_depth, min_samples_split, n_estimators, min_samples_lead, max_features, and criterion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2200 candidates, totalling 11000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2170 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3034 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3520 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4600 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5194 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5824 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6490 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7930 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8704 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9514 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10360 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11000 out of 11000 | elapsed: 31.3min finished\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"max_depth\": [2,3,4,5,6,7,8,9,10,11,12]\n",
    "            ,\"min_samples_split\" :[2,3,4,5,6]\n",
    "            ,\"n_estimators\" : [10]\n",
    "            ,\"min_samples_leaf\": [1,2,3,4,5]\n",
    "            ,\"max_features\": (4,5,6,\"sqrt\")\n",
    "            ,\"criterion\": ('gini','entropy')}\n",
    "\n",
    "rf_regr = RandomForestClassifier()\n",
    "model = GridSearchCV(rf_regr, parameters, scoring='roc_auc', cv = 5, n_jobs=-1, verbose=5)\n",
    "model_fit = model.fit(processed_X,y)\n",
    "\n",
    "learned_parameters = model_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth = learned_parameters[\"max_depth\"]\n",
    "                            ,max_features = learned_parameters['max_features']\n",
    "                            ,min_samples_leaf = learned_parameters['min_samples_leaf']\n",
    "                            ,min_samples_split = learned_parameters['min_samples_split']\n",
    "                            ,criterion = learned_parameters['criterion']\n",
    "                            ,n_estimators = 5000\n",
    "                            ,n_jobs = -1)\n",
    "rfc.fit(processed_X,y)\n",
    "ypred_test = rfc.predict_proba(processed_y)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['Y'] = ypred_test[:,1]\n",
    "sub.to_csv('subRF1a.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Plugging in these parameters into the model and training then predicting the submissions yieleded a **0.86066** AUC score from Kaggle. Better than Logistic Regression, but worse than my XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "Then on this new preprocessed train and test set I applied XGBoost referencing [this](https://www.kaggle.com/phunter/xgboost-with-gridsearchcv) and [this](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/). I applied grid-search piece by piece, looking for the best max_depth and min_child_weight, then gamma and learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 20.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'min_child_weight': 5}, 0.8597534544340567)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tuning max_depth and min_child\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5, verbose=5)\n",
    "gsearch1.fit(processed_X, y)\n",
    "gsearch1.best_params_, gsearch1.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "xgb1.fit(processed_X,y)\n",
    "ypred_test = xgb1.predict_proba(processed_y)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['Y'] = ypred_test[:,1]\n",
    "sub.to_csv('subXGB1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Results\n",
    "I was curious about the results of this grid-search so I submitted it onto Kaggle -- **0.86493** AUC score. Not too bad, better than Logistic Regression and Random Forest, but not as good as when I had meticulously hand-tuned my first XGBoost submissions. Maybe more tuning will help. I moved onto tuning gamma and learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=4)]: Done 245 out of 245 | elapsed: 44.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.0},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.0},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.0},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.0},\n",
       "  mean: 0.85975, std: 0.00770, params: {'learning_rate': 0.1, 'gamma': 0.0},\n",
       "  mean: 0.85803, std: 0.00761, params: {'learning_rate': 0.15, 'gamma': 0.0},\n",
       "  mean: 0.85647, std: 0.00733, params: {'learning_rate': 0.2, 'gamma': 0.0},\n",
       "  mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.1},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.1},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.1},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.1},\n",
       "  mean: 0.85975, std: 0.00770, params: {'learning_rate': 0.1, 'gamma': 0.1},\n",
       "  mean: 0.85804, std: 0.00762, params: {'learning_rate': 0.15, 'gamma': 0.1},\n",
       "  mean: 0.85647, std: 0.00733, params: {'learning_rate': 0.2, 'gamma': 0.1},\n",
       "  mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.2},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.2},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.2},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.2},\n",
       "  mean: 0.85978, std: 0.00767, params: {'learning_rate': 0.1, 'gamma': 0.2},\n",
       "  mean: 0.85804, std: 0.00762, params: {'learning_rate': 0.15, 'gamma': 0.2},\n",
       "  mean: 0.85675, std: 0.00763, params: {'learning_rate': 0.2, 'gamma': 0.2},\n",
       "  mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.3},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.3},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.3},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.3},\n",
       "  mean: 0.85975, std: 0.00766, params: {'learning_rate': 0.1, 'gamma': 0.3},\n",
       "  mean: 0.85796, std: 0.00749, params: {'learning_rate': 0.15, 'gamma': 0.3},\n",
       "  mean: 0.85691, std: 0.00773, params: {'learning_rate': 0.2, 'gamma': 0.3},\n",
       "  mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.4},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.4},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.4},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.4},\n",
       "  mean: 0.85976, std: 0.00766, params: {'learning_rate': 0.1, 'gamma': 0.4},\n",
       "  mean: 0.85824, std: 0.00714, params: {'learning_rate': 0.15, 'gamma': 0.4},\n",
       "  mean: 0.85645, std: 0.00827, params: {'learning_rate': 0.2, 'gamma': 0.4},\n",
       "  mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.5},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.5},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.5},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.5},\n",
       "  mean: 0.85975, std: 0.00765, params: {'learning_rate': 0.1, 'gamma': 0.5},\n",
       "  mean: 0.85808, std: 0.00701, params: {'learning_rate': 0.15, 'gamma': 0.5},\n",
       "  mean: 0.85635, std: 0.00821, params: {'learning_rate': 0.2, 'gamma': 0.5},\n",
       "  mean: 0.84589, std: 0.00473, params: {'learning_rate': 0.01, 'gamma': 0.6},\n",
       "  mean: 0.85140, std: 0.00613, params: {'learning_rate': 0.02, 'gamma': 0.6},\n",
       "  mean: 0.85416, std: 0.00615, params: {'learning_rate': 0.025, 'gamma': 0.6},\n",
       "  mean: 0.85551, std: 0.00672, params: {'learning_rate': 0.03, 'gamma': 0.6},\n",
       "  mean: 0.85978, std: 0.00767, params: {'learning_rate': 0.1, 'gamma': 0.6},\n",
       "  mean: 0.85801, std: 0.00697, params: {'learning_rate': 0.15, 'gamma': 0.6},\n",
       "  mean: 0.85640, std: 0.00827, params: {'learning_rate': 0.2, 'gamma': 0.6}],\n",
       " {'gamma': 0.2, 'learning_rate': 0.1},\n",
       " 0.8597785718967739)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,7)],\n",
    " 'learning_rate':[0.01, 0.02, 0.025, 0.03, 0.1, 0.15, 0.2]\n",
    "\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5, verbose=5)\n",
    "gsearch3.fit(processed_X, y)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=5, gamma=0.2, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "xgb3.fit(processed_X, y)\n",
    "ypred_test = xgb3.predict_proba(processed_y)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['Y'] = ypred_test[:,1]\n",
    "sub.to_csv('subsXGB3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Secondary Results\n",
    "With these results, the submission yielded a **0.86493** AUC score from Kaggle. This was the same as the other submission! Interesting. I could see the power behind grid-search but it takes so dang long. At this point in the competition I didn't have much time to fine-tune it so I moved on to try another technique.\n",
    "\n",
    "#### Ensembling\n",
    "Taking the predicted results from the three models, I wanted to apply XGBoost on top of this ensemble. Using different models to predict data I hoped that combining them would account for the weaknesses and strengths of each model to produce a more successful result. First however, I needed to create a new set of training data. I used stratified kfold to generate folds within my preprocessed data and trained/tested for new results based on the Logistic Regression, Random Forest, and XGBoost models I had used above to create the predicted results.\n",
    "\n",
    "##### Creating the Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataframe to hold new training set\n",
    "ensem = pd.DataFrame()\n",
    "\n",
    "ensem['logr'] = np.zeros_like(train_id)\n",
    "ensem['ranf'] = np.zeros_like(train_id)\n",
    "ensem['xgb'] = np.zeros_like(train_id)\n",
    "\n",
    "ensem['idx'] = train_id\n",
    "ensem = ensem.set_index('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data and test and train based on the models used above\n",
    "for (train, test) in skf.split (processed_X, y):\n",
    "    xtrain = processed_X.iloc[train]\n",
    "    xtest = processed_X.iloc[test]\n",
    "    ytrain = y[train]\n",
    "    \n",
    "    ensem_lr = LogisticRegression(penalty = 'l1', C=0.1, n_jobs=-1)\n",
    "\n",
    "    ensem_rfc = RandomForestClassifier(max_depth = 10\n",
    "                            ,max_features = 'sqrt'\n",
    "                            ,min_samples_leaf = 2\n",
    "                            ,min_samples_split = 2\n",
    "                            ,criterion = 'entropy'\n",
    "                            ,n_estimators = 10\n",
    "                            ,n_jobs = -1)\n",
    "    \n",
    "    ensem_xgb3 = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    "    min_child_weight=5, gamma=0.2, subsample=0.8, colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "    \n",
    "    ensem_lr.fit(xtrain, ytrain)\n",
    "    ensem_rfc.fit(xtrain, ytrain)\n",
    "    ensem_xgb3.fit(xtrain, ytrain)\n",
    "    \n",
    "# fill into train datafram\n",
    "    ensem.iloc[test, 0] = (ensem_lr.predict_proba(xtest)[:,1])\n",
    "    ensem.iloc[test, 1] = (ensem_rfc.predict_proba(xtest)[:,1])\n",
    "    ensem.iloc[test, 2] = (ensem_xgb3.predict_proba(xtest)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put together test data results\n",
    "ensem_y = pd.DataFrame()\n",
    "\n",
    "ensem_y['logr'] = ensem_lr.predict_proba(processed_y)[:,1]\n",
    "ensem_y['ranf'] = ensem_rfc.predict_proba(processed_y)[:,1]\n",
    "ensem_y['xgb'] = ensem_xgb3.predict_proba(processed_y)[:,1]\n",
    "\n",
    "ensem_y['idx'] = test_id\n",
    "ensem_y = ensem_y.set_index('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logr</th>\n",
       "      <th>ranf</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048960</td>\n",
       "      <td>0.023159</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016549</td>\n",
       "      <td>0.012434</td>\n",
       "      <td>0.004198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102611</td>\n",
       "      <td>0.101756</td>\n",
       "      <td>0.120191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029428</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.021824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020677</td>\n",
       "      <td>0.050055</td>\n",
       "      <td>0.042095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         logr      ranf       xgb\n",
       "idx                              \n",
       "1    0.048960  0.023159  0.027800\n",
       "2    0.016549  0.012434  0.004198\n",
       "3    0.102611  0.101756  0.120191\n",
       "4    0.029428  0.025134  0.021824\n",
       "5    0.020677  0.050055  0.042095"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logr</th>\n",
       "      <th>ranf</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.149194</td>\n",
       "      <td>0.212361</td>\n",
       "      <td>0.149641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>0.021118</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.006431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>0.016782</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.006382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50002</th>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.037562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50003</th>\n",
       "      <td>0.045361</td>\n",
       "      <td>0.096430</td>\n",
       "      <td>0.065868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           logr      ranf       xgb\n",
       "idx                                \n",
       "49999  0.149194  0.212361  0.149641\n",
       "50000  0.021118  0.009416  0.006431\n",
       "50001  0.016782  0.005733  0.006382\n",
       "50002  0.019481  0.024560  0.037562\n",
       "50003  0.045361  0.096430  0.065868"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensem_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost and Grid-Search on the Ensemble\n",
    "I applied similar techniques from above on the ensemble to find the parameters for the XGBoost model using my new training and test sets. Instead this time I also calculated the local AUC score and examined the variance of the results before I submitted onto Kaggle.\n",
    "\n",
    "First, tuning for max_depth and min_child_weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   57.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'min_child_weight': 3}, 0.8585557045305536)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'max_depth':range(3,10,1),\n",
    " 'min_child_weight':range(1,6,1)\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test4, scoring='roc_auc',n_jobs=-1,iid=False, cv=5, verbose=5)\n",
    "gsearch4.fit(ensem, y)\n",
    "gsearch4.best_params_, gsearch4.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858287635941 0.00786573295246\n"
     ]
    }
   ],
   "source": [
    "var = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "results = cross_val_score(var, ensem, y, scoring ='roc_auc', cv=5)\n",
    "print results.mean(), results.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the max_depth and min_child_weight gave a local result that was lower than my results from the public leaderboard from just the first XGBoost on processed, unensembled data. Perhaps more grid-search tuning will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   51.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'gamma': 0.3, 'subsample': 0.7}, 0.8584540722729062)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)],\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test5, scoring='roc_auc',n_jobs=-1,iid=False, cv=5, verbose=5)\n",
    "gsearch5.fit(ensem, y)\n",
    "gsearch5.best_params_, gsearch5.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858328381414 0.00736395846966\n"
     ]
    }
   ],
   "source": [
    "var = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=2, gamma=0.1, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "results = cross_val_score(var, ensem, y, scoring ='roc_auc', cv=5)\n",
    "print results.mean(), results.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the by gamma and subsample and colsample_bytree improved the local score a bit but not by much. Let's just plug it into Kaggle and see what the public score says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb4 = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=2, gamma=0.1, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "xgb4.fit(ensem,y)\n",
    "ypred_test = xgb4.predict_proba(ensem_y)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['Y'] = ypred_test[:,1]\n",
    "sub.to_csv('XGB4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Result\n",
    "\n",
    "The Kaggle AUC score for ensembling the three models with XGBoost had a local estimate of 0.838328 but it actually performed better with a score of **0.86347**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "* My first attempts with hand-tuning the crap out of a single XGBoost model based on Kaggle responses gave me the best results (and also overconfidence and a false sense of security). I was able to get into second position on the public leaderboard with this method.\n",
    "* After preprocessing, using grid-search, logistic regression and random forest models, and ensembling, the AUC scores did not improve.\n",
    "\n",
    "After the private leaderboard was available, my position dropped from 2 to 18, much to my display. I realized I had probably overfitted because my first method relied so much on using the public scores from Kaggle to tune my XGBoost model.\n",
    "\n",
    "If I were to do this again, I would want to spend more time meticulously tuning the parameters on the models logistic regression, random forest, and XGBoost models I used in my ensemble. And then spend more time tuning the parameters on the XGBoost I used on the ensemble. I would also utilize the ability to check the local AUC score of the model and would like to analyze how the local AUC scores differ from the public Kaggle scores.\n",
    "\n",
    "All in all, I am proud of my progress and how much I learned through this assignment. Though it was stressful at times, it was also fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
